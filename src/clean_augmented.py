import os

def clean_augmented_files():
    """–£–¥–∞–ª—è–µ–º –≤—Å–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã"""
    
    for split in ['train', 'valid']:
        images_dir = f'dataset/{split}/images'
        labels_dir = f'dataset/{split}/labels'
        
        # –£–¥–∞–ª—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        for file in os.listdir(images_dir):
            if file.startswith('aug_'):
                os.remove(os.path.join(images_dir, file))
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω: {file}")
        
        # –£–¥–∞–ª—è–µ–º –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–∞–∑–º–µ—Ç–∫–∏
        for file in os.listdir(labels_dir):
            if file.startswith('aug_'):
                os.remove(os.path.join(labels_dir, file))
                print(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω: {file}")
    
    print("‚úÖ –í—Å–µ –∞—É–≥–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã —É–¥–∞–ª–µ–Ω—ã!")

def check_dataset_size():
    """–ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏"""
    from collections import Counter
    
    class_names = ['Burn Mark', 'Coating_defects', 'Crack', 'EROSION']
    class_counts = Counter()
    
    for split in ['train', 'valid']:
        labels_dir = f'dataset/{split}/labels'
        
        for label_file in os.listdir(labels_dir):
            with open(os.path.join(labels_dir, label_file), 'r') as f:
                for line in f:
                    class_id = int(line.strip().split()[0])
                    class_counts[class_id] += 1
    
    print("\nüìä –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏:")
    total = sum(class_counts.values())
    for class_id, count in class_counts.items():
        percentage = (count / total) * 100
        print(f"   {class_names[class_id]}: {count} ({percentage:.1f}%)")

if __name__ == "__main__":
    clean_augmented_files()
    check_dataset_size()